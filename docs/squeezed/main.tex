%

\documentclass{article}

\usepackage{a4wide}
\usepackage{graphics}
\usepackage{longtable}
\usepackage{fancyhdr}
\usepackage{url}

\newcommand{\eqdef}{\stackrel{def}{=}}

\setlength\topskip{0cm}
\setlength\topmargin{0cm}
\setlength\oddsidemargin{0cm}
\setlength\evensidemargin{0cm}
%\setlength\parindent{0pt}

\input{globals}

\begin{document}

% The coversheet itself
\include{coversheet}

% ... and off we go!

\section{Introduction}

We wish to:
\begin{enumerate}
\item allow VM memory to be adjusted dynamically without having to reboot; and
\item ``squeeze'' a few more VMs onto a host to cover the interval between another host failing and more capacity being brought online.
\end{enumerate}

\squeezed{} is a per-host memory ballooning daemon. It performs two tasks:
\begin{enumerate}
\item it exports a simple host memory management interface to the \xapi{} toolstack through which \xapi{} can {\em reserve} memory for new domains;
\item it applies a {\em ballooning policy} to all domains running on a host.
\end{enumerate}
The daemon currently includes a simple ballooning policy (see Section~\ref{Ballooning policy}) and the intention is that this can be replaced later with more sophisticated policies (e.g.\ {\em xenballoond}\footnote{\url{http://wiki.xensource.com/xenwiki/Open_Topics_For_Discussion?action=AttachFile&do=get&target=Memory+Overcommit.pdf}})). Although the only client is the \xapi{} toolstack, the interface can in theory be used by other clients.

In the short-term this document will allow the assumptions and the design to be reviewed. In the longer term this document will become part of the \xapi{} toolstack software design notes.

The rest of this document is structured as follows. Section~\ref{assumptions} lists assumptions made by the ballooning daemon on other parts of the system; these assumptions need careful review and may not be valid. Section~\ref{Toolstack interface} describes the interface between the toolstack and the ballooning daemon. Section~\ref{Ballooning policy} describes the simple built-in ballooning policy and Section~\ref{memory model} describes how \squeezed{} models memory. The main loop of the daemon is described in Section~\ref{The main loop} and a detailed example is described in Section~\ref{example}. Section~\ref{structure} describes the structure of the daemon itself and finally Section~\ref{issues} lists some known issues.

\section{Environmental assumptions}
\label{assumptions}
\begin{enumerate}
\item The \squeezed{} daemon runs within a \xen{} domain 0 and communicates to xenstored via a Unix domain socket. Therefore \squeezed{} is granted full access to xenstore, enabling it to modify every domain's \texttt{memory/target}.

\item The \squeezed{} daemon calls \texttt{setmaxmem} in order to cap the amount of memory a domain can use. This relies on a patch to xen\footnote{\url{http://xenbits.xen.org/xapi/xen-3.4.pq.hg?file/c01d38e7092a/max-pages-below-tot-pages}} which allows \texttt{maxmem}
 to be set lower than \texttt{totpages}. See Section~\ref{maxmem} for more information.

\item The \squeezed{} daemon assumes that only domains which write \texttt{control/feature-balloon} into xenstore can respond to ballooning requests. It will not ask any other domains to balloon.

\item The \squeezed{} daemon assumes that the memory used by a domain is: (i) that listed in \texttt{domain\_getinfo} as \texttt{totpages}; (ii) shadow as given by \texttt{shadow\_allocation\_get}; and (iii) a small (few KiB) of miscellaneous \xen{} structures (e.g.\ for domains, vcpus) which are invisible.

\item The \squeezed{} daemon assumes that a domain which is created with a particular \texttt{memory/target} (and \texttt{startmem}, to within rounding error) will reach a stable value of \texttt{totpages} before writing \texttt{control/feature-balloon}.\footnote{%
The \texttt{control/feature-balloon} key is probably the wrong signal.
}%
The daemon writes this value to \texttt{memory/memory-offset} for future reference.
\begin{itemize}
\item The \squeezed{} daemon does not know or care exactly what causes the difference between \texttt{totpages} and \texttt{memory/target} and it does {\em not} expect it to remain constant across \xen{} releases. It only expects the value to remain constant over the lifetime of a domain.
\end{itemize}

\item The \squeezed{} daemon assumes that the balloon driver has hit its target when difference between \texttt{memory/target} and \texttt{totpages} equals the \texttt{memory-offset} value.
\begin{itemize}
\item Corrollary: to make a domain with a responsive balloon driver currenty using \texttt{totpages} allocate or free $x$, it suffices to set \texttt{memory/target} to $x+\texttt{totpages}+\texttt{memory-offset}$ and wait for the balloon driver to finish. See Section~\ref{memory model} for more detail.
\end{itemize}
\item The \squeezed{} daemon must maintain a ``slush fund'' of memory (currently 9MiB) which it must prevent any domain from allocating. Since (i) some \xen{} operations (such as domain creation) require memory within a physical address range (e.g. $<$ 4GiB) and (ii) since \xen{} preferentially allocates memory outside these ranges, it follows that by preventing guests from allocating {\em all} host memory (even transiently) we guarantee that memory from within these special ranges is always available. See Section~\ref{twophase section} for more details.

\item The \squeezed{} daemon assumes that it may set \texttt{memory/target} to any value within range: \texttt{memory/dynamic-max} to \texttt{memory/dynamic-min}

\item The \squeezed{} daemon assumes that the probability of a domain booting successfully may be increased by setting \texttt{memory/target} closer to \texttt{memory/static-max}.

\item The \squeezed{} daemon assumes that, if a balloon driver has not made any visible progress after 5 seconds, it is effectively {\em inactive}. Active domains will be expected to pick up the slack.
\end{enumerate}

\section{Toolstack interface}
\label{Toolstack interface}

This section begins by describing the concept of a {\em reservation} and then describes the toolstack interface in pseudocode.

A {\em reservation} is: an amount of host free memory tagged with an associated {\em reservation id}.
Note this is an internal \squeezed{} concept and \xen{} is completely unaware of it. When the daemon is moving memory between domains, it always aims to keep 
\[
\mathit{host\ free\ memory} >= s + \sum_i{\mathit{reservation}_i}
\]
where $s$ is the size of the ``slush fund'' (currently 9MiB) and $\mathit{reservation}_i$ is the amount corresponding to the $i$th reservation.

As an aside:
Earlier versions of \squeezed{} always associated memory with a \xen{} domain. Unfortunately this required domains to be created before memory was freed which was problematic because domain creation requires small amounts of contiguous frames. Rather than implement some form of memory defragmentation, \squeezed{} and \xapi{} were modified to free memory before creating a domain. This necessitated making memory {\em reservations} first-class stand-alone entities.


Once a {\em reservation} is made (and the corresponding memory is freed), it can be {\em transferred} to a domain created by a toolstack. This associates the {\em reservation} with that domain so that, if the domain is destroyed, the {\em reservation} is also freed. Note that \squeezed{} is careful not to count both a domain's {\em reservation} and its \texttt{totpages} during e.g. domain building: instead it considers the domain's allocation to be the maximum of {\em reservation} and \texttt{totpages}.

The size of a {\em reservation} may either be specified exactly by the caller or the caller may provide a memory range. If a range is provided the daemon will allocate at least as much as the minimum value provided and as much as possible up to the maximum. By allocating as much memory as possible to the domain, the probability of a successful boot is increased.

Clients of the \squeezed{} provide a string name when they log in. All untransferred reservations made by a client are automatically deleted when a client logs in. This prevents memory leaks where a client crashes and loses track of its own reservation ids.

The interface looks like this:
\begin{verbatim}
string session_id login(string client_name)

string reservation_id reserve_memory(string client_name, int kib)
int amount, string reservation_id reserve_memory_range(string client_name, int min, int max)

void delete_reservation(string client_name, string reservation_id)

void transfer_reservation_to_domain(string client_name, string reservation_id, int domid)
\end{verbatim}

The \xapi{} toolstack has code like the following: (in \url{http://www.xen.org/files/XenCloud/ocamldoc/index.html?c=xapi&m=Vmops})
\begin{verbatim}
 r_id = reserve_memory_range("xapi", min, max);
 try:
    d = domain_create()
    transfer_reservation_to_domain("xapi", r_id, d)
 with:
    delete_reservation("xapi", r_id)
\end{verbatim}

The interface is currently implemented using a trivial RPC protocol over xenstore where requests and responses are directories and their parameters and return values are keys in those directories.

\section{Ballooning policy}
\label{Ballooning policy}
This section describes the very simple default policy currently built-into \squeezed{}.

Every domain has a pair of values written into xenstore: \texttt{memory/dynamic-min} and \texttt{memory/dynamic-max} with the following meanings:
\begin{description}
\item[\texttt{memory/dynamic-min}]: the lowest value that \squeezed{} is allowed to set \texttt{memory/target}. The administrator should make this as low as possible but high enough to ensure that the applications inside the domain actually work.
\item[\texttt{memory/dynamic-max}]: the highest value that \squeezed{} is allowed to set \texttt{memory/target}. This can be used to dynamically cap the amount of memory a domain can use.
\end{description}
If all balloon drivers are responsive then \squeezed{} daemon allocates memory proportionally, so that each domain has the same value of:
\[
\frac{
\texttt{memory/target}-\texttt{memory/dynamic-min}
}{
\texttt{memory/dynamic-max}-\texttt{memory/dynamic-min}
}
\]
So:
\begin{itemize}
\item if memory is plentiful then all domains will have $\texttt{memory/target}=\texttt{memory/dynamic-max}$
\item if memory is scarce then all domains will have $\texttt{memory/target}=\texttt{memory/dynamic-min}$
\end{itemize}
Note that the values of \texttt{memory/target} suggested by the policy are ideal values. In many real-life situations (e.g. when a balloon driver fails to make progress and is declared {\em inactive}) the \texttt{memory/target} values will be different.

Note that, by default, domain 0 has $\texttt{dynamic\_min}=\texttt{dynamic\_max}$, effectively disabling ballooning.  Clearly a more sophisticated policy would be required here since ballooning down domain 0 as extra domains are started would be\ldots{} problematic.

\section{The memory model used by \squeezed{}}
\label{memory model}
This section describes the model used internally by \squeezed{}.

The \squeezed{} daemon considers a ballooning-aware domain (i.e.\ one which has written the \texttt{feature-balloon} flag into xenstore) to be a 6-tuple:
\[
\mathit{ballooning~domain} = (\texttt{dynamic-min}, \texttt{dynamic-max}, \texttt{target}, \texttt{totpages}, \texttt{memory-offset}, \texttt{maxmem})
\]
where
\begin{description}
\item[\texttt{dynamic-min}]: policy value written to \texttt{memory/dynamic-min} in xenstore by a toolstack (see Section~\ref{Ballooning policy})
\item[\texttt{dynamic-max}]: policy value written to \texttt{memory/dynamic-max} in xenstore by a toolstack (see Section~\ref{Ballooning policy})
\item[\texttt{target}]: balloon driver target written to \texttt{memory/target} in xenstore by \squeezed{}
\item[\texttt{totpages}]: instantaneous number of pages used by the domain as returned by \texttt{domain\_getinfo}
\item[\texttt{memory-offset}]: constant difference between \texttt{target} and \texttt{totpages} when the balloon driver believes no ballooning is necessary:
\[
\texttt{memory-offset} \eqdef \texttt{totpages} - \texttt{target} \mathit{~when~idle}
\]
\item[\texttt{maxmem}]: upper limit on \texttt{totpages}:
\[
\texttt{totpages} <= \texttt{maxmem}
\]
\end{description}
For convenience we define a \texttt{totpages'} to be the target value necessary to cause a domain currently using \texttt{totpages} to maintain this value indefinitely.
\[
\texttt{totpages'} \eqdef \texttt{totpages} - \texttt{memory-offset}
\]

The \squeezed{} daemon believes that:
\begin{itemize}
\item a domain should be ballooning iff $\texttt{totpages'} <> \texttt{target}$ (unless it has become {\em inactive});
\item a domain has hit its target iff $\texttt{totpages'} = \texttt{target}$ (to within 1 page);
\item if a domain has $\texttt{target}\leftarrow x$ then, when ballooning is complete, it will have $\texttt{totpages}=\texttt{memory-offset}+x$; and therefore
\item to cause a domain to free $y$ it sufficies to set $\texttt{target}\leftarrow\texttt{totpages}-\texttt{memory-offset}-y$.
\end{itemize}

The \squeezed{} daemon considers non-ballooning aware domains (i.e.\ those which have not written \texttt{feature-balloon}) to be represented by pairs of:
\[
\mathit{other~domain} = (\texttt{totpages}, \mathit{reservation})
\]
where
\begin{description}
\item[\texttt{totpages}]: instantaneous number of pages used by the domain
  as returned by \texttt{domain\_getinfo}
\item[$\mathit{reservation}$]: memory initially freed for this domain by
  \squeezed{} after a \texttt{transfer\_reservation\_to\_domid} call
\end{description}
Note that non-ballooning aware domains will always have
$\texttt{startmem}=\texttt{target}$ since the domain will not be instructed
to balloon. Since a domain which is being built will have
$0<=\texttt{totpages}<=\mathit{reservation}$, \squeezed{} computes:
\[
\mathit{unused}(i) \eqdef i.\mathit{reservation} - i.\texttt{totpages}
\]
and subtracts this from its model of the host's free memory, ensuring that it doesn't accidentally reallocate this memory for some other purpose.


The \squeezed{} daemon believes that:
\begin{itemize}
\item all guest domains start out as non-ballooning aware domains where $\texttt{target}=\mathit{reservation}=\texttt{startmem}$;
\item some guest domains become ballooning-aware during their boot sequence i.e.\ when they write \texttt{feature-balloon}
\end{itemize}

The \squeezed{} daemon considers a host to be a 5-tuple:
\[
\mathit{host} = (\mathit{ballooning~domains}, \mathit{other~domains}, s, \texttt{physinfo.free\_pages}, \mathit{reservation}_i)
\]
where
\begin{description}
\item[$\mathit{ballooning~domains}$]: a list of $\mathit{ballooning~domain}$
  values representing domains which \squeezed{} will instruct to balloon;
\item[$\mathit{other~domains}$]: a list of $\mathit{other~domain}$ values
  which includes both domains which are still booting and will transform
  into $\mathit{ballooning~domains}$ and those which have no balloon drivers.
\item[$s$]: a ``slush fund'' of low memory required for \xen{};
\item[\texttt{physinfo.free\_pages}]: total amount of memory instantanously free (including both \texttt{free\_pages} and \texttt{scrub\_pages})
\item[$\mathit{reservation}_i$]: a set of memory {\em reservations} not allocated to any domain
\end{description}
The \squeezed{} daemon considers memory to be unused (i.e.\ not allocated for any useful purpose) as follows:
\[
\mathit{unused~memory} = \texttt{physinfo.free\_pages} -
\Sigma_i\mathit{reservation}_i - s - \Sigma_{i\in\mathit{other~domains}}\mathit{unused}(i)
\]

\section{The main loop}
\label{The main loop}

The main loop \footnote{\texttt{change\_host\_free\_memory} in \url{http://xenbits.xen.org/xapi/xen-api.hg?file/3e8c0167940d/ocaml/xenops/squeeze.ml}}  is triggered by either:
\begin{enumerate}
\item the arrival of an allocation request on the toolstack interface; or
\item the policy engine -- polled every 10s -- deciding that a target adjustment is needed.
\end{enumerate}
Each iteration of the main loop\footnote{\texttt{one\_iteration} in \url{http://xenbits.xen.org/xapi/xen-api.hg?file/3e8c0167940d/ocaml/xenops/squeeze.ml}} generates the following actions:
\begin{enumerate}
\item Domains which were active but have failed to make progress towards their target in 5s are declared {\em inactive}. These domains then have:
\[
\texttt{maxmem}\leftarrow\mathit{min}(\texttt{target}, \texttt{totpages})
\]
\item Domains which were inactive but have started to make progress towards their target are declared {\em active}. These domains then have:
\[
\texttt{maxmem}\leftarrow\texttt{target}
\]
\item Domains which are currently active have new targets computed according to the policy (see Section~\ref{Ballooning policy}). Note that inactive domains are ignored and not expected to balloon.
\end{enumerate}
Note that domains remain classified as {\em inactive} only during one run of the main loop. Once the loop has terminated all domains are optimistically assumed to be {\em active} again. Therefore should a domain be classified as {\em inactive} once, it will get many later chances to respond.

See Section~\ref{twophase section} for more detail on how targets are updated and Section~\ref{maxmem} for more detail about \texttt{maxmem}.

The main loop has a notion of a host free memory ``target'', similar to the existing domain memory \texttt{target}. When we are trying to free memory (e.g. for starting a new VM), the host free memory ``target'' is increased. When we are trying to distribute memory among guests (e.g.\ after a domain has shutdown and freed lots of memory), the host free memory ``target'' is low. Note the host free memory ``target'' is always at least several MiB to ensure that some host free memory  with physical address $<$ 4GiB is free (see Section~\ref{twophase section} for related information).

The main loop terminates when all {\em active} domains have reached their targets (this could be because all domains responded or because they all wedged and became inactive); and the policy function hasn't suggested any new target changes. There are three possible results:
\begin{enumerate}
\item Success if the host free memory is near enough its ``target'';
\item Failure if the operation is simply impossible within the policy limits (i.e. \texttt{dynamic\_min} values are too high;
\item Failure if the operation failed because one or more domains became {\em inactive} and this prevented us from reaching our host free memory ``target''.
\end{enumerate}
Note that, since only {\em active} domains have their targets set, the system effectively rewards domains which refuse to free memory ({\em inactive}) and punishes those which do free memory ({\em active}). This effect is countered by signalling to the admin which domains/VMs aren't responding so they can take corrective action.
To achieve this, the daemon monitors the list of {\em inactive} domains and if a domain is {\em inactive} for more than 20s it writes a flag into xenstore \texttt{memory/uncooperative}. This key is seen by the \xapi{} toolstack which currently generates an alert to inform the admin.


\subsection{Two-phase target setting}
\label{twophase section}

\begin{figure}
\begin{center}
\includegraphics{fig/twophase}
\end{center}
\caption{The diagram shows how a system with two domains can evolve if domain \texttt{memory/target} values are increased for some domains and decreased for others, at the same time. Each graph shows two domains (domain 1 and domain 2) and a host. For a domain, the square box shows its $\texttt{totpages'}$ and the arrow indicates the direction of the \texttt{memory/target}. For the host the square box indicates total free memory. Note the highlighted state where the host's free memory is temporarily exhausted.}
\label{twophase}
\end{figure}

Consider the scenario shown graphically in Figure~\ref{twophase}. In the initial state (at the top of the diagram), there are two domains, one which has been requested to use more memory and the other requested to use less memory. In effect the memory is to be transferred from one domain to the other. In the final state (at the bottom of the diagram), both domains have reached their respective targets, the memory has been transferred and the host free memory is at the same value it was initially. However the system will not move atomically from the initial state to the final: there are a number of possible transient in-between states, two of which have been drawn in the middle of the diagram. In the left-most transient state the domain which was asked to {\em free} memory has freed all the memory requested: this is reflected in the large amount of host memory free. In the right-most transient state the domain which was asked to {\em allocate} memory has allocated all the memory requested: now the host's free memory has hit zero. 

If the host's free memory hits zero then \xen{} has been forced to give all memory to guests, including memory $<$ 4GiB which is critical for allocating certain structures. Even if we ask a domain to free memory via the balloon driver there is no guarantee that it will free the {\em useful} memory. This leads to an annoying failure mode where operations such as creating a domain free due to \texttt{ENOMEM} despite the fact that there is apparently lots of memory free.

The solution to this problem is to adopt a two-phase \texttt{memory/target} setting policy. The \squeezed{} daemon forces domains to free memory first before allowing domains to allocate, in-effect forcing the system to move through the left-most state in the diagram above.


\subsection{Use of \texttt{maxmem}}
\label{maxmem}

The \xen{} domain \texttt{maxmem} value is used to limit memory allocations by the domain. The rules are:
\begin{enumerate}
\item if the domain has never been run and is paused then $\texttt{maxmem}\leftarrow\texttt{reservation}$ (for information about reservations see Section~\ref{Toolstack interface});
\begin{itemize}
\item these domains are probably still being built and we must let them allocate their \texttt{startmem}
\item \textbf{FIXME}: this ``never been run' concept pre-dates the \texttt{feature-balloon} flag: perhaps we should use the \texttt{feature-balloon} flag instead.
\end{itemize}
\item if the domain is running and the balloon driver is thought to be working then $\texttt{maxmem}\leftarrow\texttt{target}$; and
\begin{itemize}
\item there may be a delay between lowering a target and the domain noticing so we prevent the domain from allocating memory when it should in fact be deallocating.
\end{itemize}
\item if the domain is running and the balloon driver is thought to be inactive then $\texttt{maxmem}\leftarrow \mathit{min}(\texttt{target}, \texttt{actual})$.
\begin{itemize}
\item if the domain is using more memory than it should then we allow it to make progress down towards its target; however
\item if the domain is using less memory than it should then we must prevent it from suddenly waking up and allocating more since we have probably just given it to someone else
\item \textbf{FIXME}: should we reduce the target to leave the domain in a neutral state instead of asking it to allocate and fail forever?
\end{itemize}
\end{enumerate}
   
\section{Example operation}
\label{example}

\begin{figure}
\begin{center}
\includegraphics{fig/calculation}
\end{center}
\caption{The diagram shows an initial system state comprising 3 domains on a single host. The state is not ideal; the domains each have the same policy settings (\texttt{dynamic-min} and \texttt{dynamic-max}) and yet are using differing values of $\texttt{totpages'}$. 
 In addition the host has more memory free than desired. The second diagram shows the result of computing ideal target values and the third diagram shows the result after targets have been set and the balloon drivers have responded.}
\label{calculation}
\end{figure}

The scenario in Figure~\ref{calculation} includes 3 domains (domain 1, domain 2, domain 3) on a host. Each of the domains has a non-ideal $\texttt{totpages'}$ value.

 Recall we also have the policy constraint that:
\[
\texttt{dynamic-min} <= \texttt{target} <= \texttt{dynamic-max}
\]
Hypothetically if we reduce \texttt{target} by $\texttt{target}-\texttt{dynamic-min}$ (i.e. by setting $\texttt{target}\leftarrow\texttt{dynamic-min}$) then we should reduce \texttt{totpages} by the same amount, freeing this much memory on the host. In the upper-most graph in Figure~\ref{calculation} the total amount of memory which would be freed if we set each of the 3 domain's $\texttt{target}\leftarrow\texttt{dynamic-min}$ is:
\[
\mathit{d1} + \mathit{d2} + \mathit{d3}
\]
In this hypothetical situation we would now have $x + s + \mathit{d1} + \mathit{d2} + \mathit{d3}$ free on the host where $s$ is the host slush fund and $x$ is completely unallocated. Since we always want to keep the host free memory above $s$, we are free to return $x + \mathit{d1} + \mathit{d2} + \mathit{d3}$ to guests. If we use the default built-in proportional policy then, since all domains have the same \texttt{dynamic-min} and \texttt{dynamic-max}, each gets the same fraction of this free memory which we call $g$:
\[
g \eqdef \frac{x + \mathit{d1} + \mathit{d2} + \mathit{d3}}{3}
\]
For each domain, the ideal balloon target is now $\texttt{target} = \texttt{dynamic-min} + g$. The \squeezed{} daemon sets these targets in two phases, as described in Section~\ref{twophase section}

\section{The structure of the daemon}
\label{structure}
The \squeezed{} daemon is a single-threaded daemon which is started by an \texttt{init.d} script. It sits waiting for incoming requests on its toolstack interface and checks every 10s whether all domain targets are set to the ideal values (see Section~\ref{Ballooning policy}). If an allocation request arrives or if the domain targets require adjusting then it calls into the module \url{ocaml/xenops/squeeze_xen.ml}\footnote{\url{http://www.xen.org/files/XenCloud/ocamldoc/index.html?c=xenops&m=Squeeze_xen}}.

The module \url{ocaml/xenops/squeeze_xen.ml} contains code which inspects the state of the host (through hypercalls and reading xenstore) and creates a set of records describing the current state of the host and all the domains. Note this snapshot of state is not atomic -- it is pieced together from multiple hypercalls and xenstore reads -- we assume that the errors generated are small and we ignore them.
 These records are passed into the \url{ocaml/xenops/squeeze.ml}\footnote{\url{http://www.xen.org/files/XenCloud/ocamldoc/index.html?c=xenops&m=Squeeze}} module where they are processed and converted into a list of {\em actions} i.e. (i) updates to \texttt{memory/target} and; (ii) declarations that particular domains have become {\em inactive} or {\em active}. The rationale for separating the \xen{} interface from the main ballooning logic was to make testing easier: the module \url{ocaml/xenops/squeeze_test.ml}\footnote{\url{http://www.xen.org/files/XenCloud/ocamldoc/index.html?c=xenops&m=Squeeze_test}} contains a simple simulator which allows various edge-cases to be checked.

\section{Issues}
\label{issues}
\begin{itemize}
\item If a linux domU kernel has the netback, blkback or blktap modules then they away pages via \texttt{alloc\_empty\_pages\_and\_pagevec()} during boot. This interacts with the balloon driver to break the assumption that, reducing the target by $x$ from a neutral value should free $x$ amount of memory.
\item Polling the state of the host (particular the xenstore contents) is a bit inefficient. Perhaps we should move the policy values \texttt{dynamic\_min} and \texttt{dynamic\_max} to a separate place in the xenstore tree and use watches instead.
\item The memory values given to the domain builder are in units of MiB. We may wish to similarly quantise the \texttt{target} value or check that the \texttt{memory-offset} calculation still works.
\item The \xen{} patch queue reintroduces the lowmem emergency pool\footnote{\url{http://xenbits.xen.org/xapi/xen-3.4.pq.hg?file/c01d38e7092a/lowmem-emergency-pool}}. This was an attempt to prevent guests from allocating lowmem before we switched to a two-phase target setting procedure. This patch can probably be removed.
\item It seems unnecessarily evil to modify an {\em inactive} domain's \texttt{maxmem} leaving $\texttt{maxmem}<\texttt{target}$, causing the guest to attempt allocations forwever. It's probably neater to move the \texttt{target} at the same time.
\item Declaring a domain {\em active} just because it makes small amounts of progress shouldn't be enough. Otherwise a domain could free 1 byte (or maybe 1 page) every 5s.
\item Likewise, declaring a domain ``uncooperative'' only if it has been {\em inactive} for 20s means that a domain could alternate between {\em inactive} for 19s and {\em active} for 1s and not be declared ``uncooperative''.
\end{itemize}
\appendix{}

\include{fdl}

\end{document}
